# ğŸ“ Sistema de AnÃ¡lisis de AtenciÃ³n en Clases

Sistema de inteligencia artificial para analizar la atenciÃ³n de estudiantes en tiempo real mediante el anÃ¡lisis de video streaming. Utiliza visiÃ³n por computadora y deep learning para detectar postura, expresiÃ³n facial y direcciÃ³n de la mirada.

## ğŸŒŸ CaracterÃ­sticas

- âœ… **AnÃ¡lisis en Tiempo Real**: Procesamiento de video streaming en vivo
- ğŸ‘¥ **Tracking Multi-Persona**: Seguimiento individual de cada estudiante
- ğŸ“Š **MÃ©tricas de AtenciÃ³n**: AnÃ¡lisis basado en:
  - DirecciÃ³n de la mirada (40%)
  - Postura corporal (30%)
  - OrientaciÃ³n facial (30%)
- ğŸ† **Ranking AutomÃ¡tico**: ClasificaciÃ³n de estudiantes por nivel de atenciÃ³n
- ğŸ“¸ **Captura de Rostros**: Registro visual de cada participante
- ğŸ¨ **Interfaz Moderna**: Dashboard interactivo en React

## ğŸ—ï¸ Arquitectura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      WebSocket      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   React App     â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚   FastAPI        â”‚
â”‚   (Frontend)    â”‚                      â”‚   (Backend)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                  â”‚
                                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
                                         â”‚                 â”‚
                                    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
                                    â”‚ MediaPipeâ”‚    â”‚  OpenCV    â”‚
                                    â”‚  Models  â”‚    â”‚ Processing â”‚
                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ InstalaciÃ³n

### Requisitos Previos

- Python 3.10+
- Node.js 16+
- npm o yarn
- CÃ¡mara web
- (Opcional) GPU NVIDIA con CUDA para mejor rendimiento

### OpciÃ³n 1: InstalaciÃ³n Local

#### Backend (Python)

```bash
# Clonar repositorio
git clone <tu-repositorio>
cd atencion-clases/backend

# Crear entorno virtual
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate

# Instalar dependencias
pip install -r requirements.txt

# Ejecutar servidor
uvicorn app.main:app --reload
```

El backend estarÃ¡ disponible en `http://localhost:8000`

#### Frontend (React)

```bash
cd ../frontend

# Instalar dependencias
npm install

# Ejecutar aplicaciÃ³n
npm start
```

El frontend estarÃ¡ disponible en `http://localhost:3000`

### OpciÃ³n 2: Docker (Recomendado)

```bash
# Desde la raÃ­z del proyecto
docker-compose up --build
```

Esto iniciarÃ¡ automÃ¡ticamente:
- Backend en `http://localhost:8000`
- Frontend en `http://localhost:3000`

## ğŸ“– Uso

### 1. Iniciar AnÃ¡lisis

1. Abre la aplicaciÃ³n en `http://localhost:3000`
2. Haz clic en **"Iniciar AnÃ¡lisis"**
3. Otorga permisos de acceso a la cÃ¡mara
4. El sistema comenzarÃ¡ a detectar y analizar estudiantes automÃ¡ticamente

### 2. Durante el AnÃ¡lisis

- VerÃ¡s estadÃ­sticas en tiempo real en el panel lateral
- Cada estudiante detectado recibirÃ¡ un ID Ãºnico
- Los porcentajes de atenciÃ³n se actualizan continuamente

### 3. Finalizar y Ver Resultados

1. Haz clic en **"Detener AnÃ¡lisis"**
2. Se generarÃ¡ automÃ¡ticamente un ranking completo
3. Cada estudiante mostrarÃ¡:
   - Foto capturada
   - Porcentaje promedio de atenciÃ³n
   - DuraciÃ³n de participaciÃ³n

### 4. Nuevo AnÃ¡lisis

Haz clic en **"Nuevo AnÃ¡lisis"** para limpiar datos y comenzar de nuevo

## ğŸ¯ API Endpoints

### WebSocket

**`WS /ws/analyze/{session_id}`**

ConexiÃ³n WebSocket para streaming en tiempo real.

**Enviar frame:**
```json
{
  "type": "frame",
  "data": "base64_encoded_image"
}
```

**Finalizar sesiÃ³n:**
```json
{
  "type": "end"
}
```

**Recibir anÃ¡lisis:**
```json
{
  "type": "analysis",
  "data": {
    "frame_number": 123,
    "total_students": 5,
    "students": [...]
  }
}
```

### REST API

**`POST /api/sessions/{session_id}/start`**
Inicia una nueva sesiÃ³n de anÃ¡lisis

**`GET /api/sessions/{session_id}/results`**
Obtiene resultados de una sesiÃ³n

**`GET /api/sessions`**
Lista sesiones activas

**`DELETE /api/sessions/{session_id}`**
Elimina una sesiÃ³n

## âš™ï¸ ConfiguraciÃ³n

Crea un archivo `.env` en `/backend` para personalizar:

```env
# API
API_HOST=0.0.0.0
API_PORT=8000

# CORS
CORS_ORIGINS=http://localhost:3000

# Video Processing
TARGET_FPS=10
VIDEO_WIDTH=1280
VIDEO_HEIGHT=720

# Detection
MIN_FACE_DETECTION_CONFIDENCE=0.5

# Attention Weights
GAZE_WEIGHT=0.40
POSTURE_WEIGHT=0.30
FACE_ORIENTATION_WEIGHT=0.30

# Thresholds
HIGH_ATTENTION_THRESHOLD=70.0
MEDIUM_ATTENTION_THRESHOLD=50.0
```

## ğŸ§  Modelos de IA Utilizados

### 1. MediaPipe Face Mesh
- **PropÃ³sito**: DetecciÃ³n facial y anÃ¡lisis de mirada
- **468 landmarks** faciales en 3D
- Tracking de iris para direcciÃ³n de mirada

### 2. MediaPipe Pose
- **PropÃ³sito**: AnÃ¡lisis de postura corporal
- 33 puntos clave del cuerpo
- DetecciÃ³n de inclinaciÃ³n y posiciÃ³n

### 3. Algoritmo de Tracking Personalizado
- **MÃ©todo**: IoU + similitud de caracterÃ­sticas
- Mantiene identidad consistente de estudiantes
- Manejo de oclusiones y movimientos

## ğŸ“Š CÃ¡lculo de AtenciÃ³n

El score de atenciÃ³n (0-100%) se calcula mediante:

```
AtenciÃ³n = (Mirada Ã— 40%) + (Postura Ã— 30%) + (OrientaciÃ³n Ã— 30%)
```

**Componentes:**

1. **Mirada (40%)**: PosiciÃ³n del iris respecto al centro del ojo
   - Mirada al frente = 100%
   - Mirada desviada = score reducido

2. **Postura (30%)**: InclinaciÃ³n y simetrÃ­a corporal
   - Postura erguida = 100%
   - Postura encorvada = score reducido

3. **OrientaciÃ³n Facial (30%)**: Ãngulo de rotaciÃ³n de la cara
   - Cara frontal = 100%
   - Cara girada = score reducido

## ğŸ¨ Estructura del Proyecto

```
atencion-clases/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py                 # API principal
â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”‚   â”œâ”€â”€ attention_analyzer.py   # Motor de anÃ¡lisis
â”‚   â”‚   â”‚   â”œâ”€â”€ face_detector.py        # DetecciÃ³n facial
â”‚   â”‚   â”‚   â””â”€â”€ tracker.py              # Sistema de tracking
â”‚   â”‚   â””â”€â”€ utils/
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ config.py
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.jsx                # Componente principal
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â””â”€â”€ services/
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md
```

## ğŸ”§ Troubleshooting

### Error: "No se pudo acceder a la cÃ¡mara"
- Verifica permisos del navegador
- AsegÃºrate de usar HTTPS o localhost
- Comprueba que la cÃ¡mara no estÃ© en uso

### WebSocket se desconecta
- Verifica que el backend estÃ© corriendo
- Revisa firewall/antivirus
- Comprueba los logs del servidor

### Bajo rendimiento
- Reduce resoluciÃ³n de video en config
- Disminuye TARGET_FPS
- Considera usar GPU con CUDA

### No detecta rostros
- Mejora iluminaciÃ³n de la sala
- Ajusta MIN_FACE_DETECTION_CONFIDENCE
- Verifica distancia de la cÃ¡mara

## ğŸš€ Optimizaciones Futuras

- [ ] IntegraciÃ³n con base de datos (MongoDB/PostgreSQL)
- [ ] Reconocimiento facial para identificaciÃ³n automÃ¡tica
- [ ] AnÃ¡lisis de emociones avanzado
- [ ] Reportes PDF exportables
- [ ] Dashboard administrativo
- [ ] API de alertas en tiempo real
- [ ] Modo multi-cÃ¡mara
- [ ] AnÃ¡lisis histÃ³rico y tendencias

## ğŸ“ Licencia

Este proyecto es de cÃ³digo abierto bajo licencia MIT.

## ğŸ‘¥ Contribuciones

Las contribuciones son bienvenidas. Por favor:

1. Fork el proyecto
2. Crea una rama para tu feature (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abre un Pull Request

## ğŸ“§ Contacto

Para preguntas o sugerencias, abre un issue en el repositorio.

---

**Nota**: Este sistema estÃ¡ diseÃ±ado para fines educativos. AsegÃºrate de cumplir con las regulaciones de privacidad y obtener consentimiento apropiado antes de usar en entornos reales.